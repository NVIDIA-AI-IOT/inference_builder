This sample demonstrates how to build a deepstream application using Inference Builder::

Model files are loaded from '/workspace/models/{MODEL_NAME}' within the container, thus the volume must be correctly mapped from the host.
For example: if you define a model with name "resnet", you must put all the model files include nvconfig, onnx, etc. to a single directory and map it to '/workspace/models/resnet' for the model to be correctly loaded.

Generate the application package and build it into a docker image:

python builder/main.py builder/samples/ds_app/ds_app.yaml -o builder/samples/ds_app --server-type serverless -t \
&& docker build -t deepstream-app builder/samples/ds_app

Run the deepstream app with a container:

docker run --rm --net=host --gpus all \
    -v $MODEL_REPO:/workspace/models \
    -v /tmp/.X11-unix/:/tmp/.X11-unix \
    -e DISPLAY=$DISPLAY \
    deepstream-app \
    --media-url /opt/nvidia/deepstream/deepstream/samples/streams/sample_1080p_h264.mp4 \
    --mime video/mp4




