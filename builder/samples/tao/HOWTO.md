# Metropolis Computer Vision Microservice HOWTO

## Introduction

This document provides instructions for building the Metropolis Computer Vision microservice and using it to run inference on images and videos.

## Prerequisites

Below packages are required to build and run the microservice:

- Docker
- Docker Compose
- NVIDIA Container Toolkit

The inference package must be generated by inference builder tool as follows:

```bash
cd ../../..
source .venv/bin/activate
pip install -r requirements.txt
python builder/main.py builder/samples/tao/ds_tao.yaml --server-type fastapi -a builder/samples/tao/openapi.yaml -o builder/samples/tao -t

```

A model-repo folder needs to be created for model drop-in:

```bash
mkdir -p ~/.cache/nim/model-repo
chmod 777 ~/.cache/nim/model-repo
mkdir ~/.cache/nim/model-repo/{NIM_MODEL_NAME}
chmod 777 ~/.cache/nim/model-repo/{NIM_MODEL_NAME}
```


## Build the microservice

The microservice uses post-processing to convert the output of the model to the format expected by the Metropolis Computer Vision endpoint, and to fetch and build the post processing library you need to have gitlab token and put it to environment variable `GITLAB_TOKEN`.

```bash
export GITLAB_TOKEN=<your_gitlab_token>
```

To build the microservice, run the following command:

```bash
cd ..
docker compose build nim-tao

```

## Run the microservice

The name of the model can be specified in the `docker-compose.yaml` file through the `NIM_MODEL_NAME` environment variable.

Before running the microservice, you need to prepare the model and drop it into the `~/.cache/nim/model_repo/{NIM_MODEL_NAME}` directory. Following files are expected to be present in the directory:

- Deepstream inference config file : `config_nvinfer.yaml`
- ONNX model file
- Label file (optional, used for post-processing)

To run the microservice, run the following command:
```bash
cd ..
docker compose up nim-tao

```

## Use the microservice

The microservice provides a REST API that can be used to run inference on images and videos.

### Run inference on an image

A sample client is available as nim_client.py, which follows the OpenAPI specification and can be used as a reference for building your own client.

```bash
python nim_client.py --port 8800 --file <path_to_image>

```

# Run the Microservice as Helm Chart

This single helm chart is able to deploy different TAO CV models, simply by updating env NIM_MODEL_NAME. No need to rebuild the helm chart.
A values override file is provided to set the NIM_MODEL_NAME; and image.repository, image.tag if you build and push a new one.

Update the helm/tao-cv-app/custom_values.yaml for:
1. NIM_MODEL_NAME, which is the TaskHead model name. It has to match the sub directory name in /opt/local-path-provisioner/pvc-*_default_local-path-pvc;
2. image.repository, image.tag if necessary.

## Prerequisites
1. UCS tools is installed. https://ucf.gitlab-master-pages.nvidia.com/docs/master/text/UCS_Installation.html
2. microk8s is installed

## Build TAO CV App Helm Chart
cd helm
make -C tao-cv-app



## Prepare models on host path for k8s

### 1. Create storageClass with name "mdx-local-path", using Local Path Provisioner
#### 1.1 Check if storageClass with name "mdx-local-path" exists
```
$ microk8s kubectl get sc
```
eg:
```
$ microk8s kubectl get sc
NAME                          PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
mdx-local-path                rancher.io/local-path   Delete          WaitForFirstConsumer   false                  141d
microk8s-hostpath (default)   microk8s.io/hostpath    Delete          WaitForFirstConsumer   false                  147d
```


#### 1.2 If not, create storageClass with name "mdx-local-path". Otherwise, skip the following steps.
```
$ curl https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.23/deploy/local-path-storage.yaml | sed 's/^  name: local-path$/  name: mdx-local-path/g' | microk8s kubectl apply -f -
```

NOTE:
1. A base path will be created at /opt/local-path-provisioner. But it won't be available until a PVC is created & the first pod accessing it.




### 2. Create PVC with default name "local-path-pvc", on the created storageClass "mdx-local-path"
##### 2.1 Check if PVC with name "local-path-pvc" exists and is on storageClass "mdx-local-path"
```
$ microk8s kubectl get pvc
```
eg:
```
$ microk8s kubectl get pvc
NAME            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
local-path-pvc   Bound    pvc-68509dc3-2f07-4e8f-8298-4a044b59546b   10Gi       RWO            mdx-local-path   141d
```

#### 2.2 If not, create PVC with default name "local-path-pvc", on the created storageClass "mdx-local-path"
```
$ curl https://raw.githubusercontent.com/rancher/local-path-provisioner/master/examples/pvc/pvc.yaml | \
    sed -e 's/storageClassName: local-path$/storageClassName: mdx-local-path/g' \
        -e 's/storage: 128Mi$/storage: 10Gi/g' | \
    microk8s kubectl apply -f -
```

NOTE: 
1. This helm chart defines volume resolving to PVC "local-path-pvc"

2. A sub directory will be created as /opt/local-path-provisioner/pvc-*_default_local-path-pvc. It won't be available until a pod accessing it.

3. You might see status "Pending" for the PVC "local-path-pvc" before the first pod accessing it.




### 3. Copy model files including model, configs to /opt/local-path-provisioner/pvc-*_default_local-path-pvc, following below structure. Make sure the each model sub directory name matches the model name in the custom_values.yaml
Eg:
```
## On host at /opt/local-path-provisioner/pvc-*_default_local-path-pvc
├── rtdetr/
│   ├── model.onnx
│   └── config_nvinfer.yml
└── segformer/
    ├── model.onnx
    └── config_nvinfer.yml

## What containers can see at /opt/nim/.cache/model-repo/
├── rtdetr/
│   ├── model.onnx
│   └── config_nvinfer.yml
└── segformer/
    ├── model.onnx
    └── config_nvinfer.yml
```
Then NIM_MODEL_NAME value in custom_values.yaml can be rtdetr or segformer.





## Run the TAO CV App helm chart in k8s
```
make -C tao-cv-app install
```

which executes the following command for you:
```
microk8s helm3 install tao-cv-app output -f custom_values.yaml
```

OR equivalently you can also run,
```
microk8s helm3 install tao-cv-app output --set tao-cv.applicationSpecs.tao-cv-deployment.containers.tao-cv-container.env[0].value=model_dir_name
```




## Stop the TAO CV App helm chart in k8s
```
make -C tao-cv-app uninstall
```

which executes the following command for you:
```
microk8s helm3 delete tao-cv-app
```
