# Metropolis Computer Vision Microservice HOWTO

## Introduction

This document provides instructions for building the Metropolis Computer Vision microservice and using it to run inference on images and videos.

## Prerequisites

Below packages are required to build and run the microservice:

- Docker
- Docker Compose
- NVIDIA Container Toolkit

The inference package must be generated by inference builder tool as follows:

```bash
cd ../../..
source .venv/bin/activate
pip install -r requirements.txt
python builder/main.py builder/samples/tao/ds_tao.yaml --server-type fastapi -a builder/samples/tao/openapi.yaml -o builder/samples/tao -t

```

A model-repo folder needs to be created for model drop-in:

```bash
mkdir -p ~/.cache/nim/model-repo
chmod 777 ~/.cache/nim/model-repo
```


## Build the microservice

The microservice uses post-processing to convert the output of the model to the format expected by the Metropolis Computer Vision endpoint, and to fetch and build the post processing library you need to have gitlab token and put it to environment variable `GITLAB_TOKEN`.

```bash
export GITLAB_TOKEN=<your_gitlab_token>
```

To build the microservice, run the following command:

```bash
cd ..
docker compose build nim-tao

```

## Run the microservice

The name of the model can be specified in the `docker-compose.yaml` file through the `NIM_MODEL_NAME` environment variable.

Before running the microservice, you need to prepare the model and drop it into the `~/.cache/nim/model_repo/{NIM_MODEL_NAME}` directory. Following files are expected to be present in the directory:

- Deepstream inference config file : `config_nvinfer.yaml`
- ONNX model file
- Label file (optional, used for post-processing)

To run the microservice, run the following command:
```bash
cd ..
docker compose up nim-tao

```

## Use the microservice

The microservice provides a REST API that can be used to run inference on images and videos.

### Run inference on an image

A sample client is available as nim_client.py, which follows the OpenAPI specification and can be used as a reference for building your own client.

```bash
python nim_client.py --port 8800 --file <path_to_image>

```

