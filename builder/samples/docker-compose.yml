################################################################################
# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
#
# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
# property and proprietary rights in and to this material, related
# documentation and any modifications thereto. Any use, reproduction,
# disclosure or distribution of this material and related documentation
# without an express license agreement from NVIDIA CORPORATION or
# its affiliates is strictly prohibited.
################################################################################

services:
  nim-vila:
    build:
      context: vila
      target: api_server
      dockerfile: Dockerfile
    image: nim-vila:0.7.0
    volumes:
      # correct the below mapping for your vila checkpoints
      - '~/.cache/nim/model-repo/vila/:/config/models/vila'

    ipc: host
    # init: true
    ports:
      - "8800-8803:8000-8003"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: 1
              device_ids: ['0']
              capabilities: [gpu]
    environment:
      DEBUG: 0
  ms-nvclip:
    build:
      context: nvclip
      target: builder_base
      dockerfile: Dockerfile
    image: ms-nvclip
    volumes:
      - '~/.cache/nim/model-repo/:/workspace/model-repo'
    ipc: host
    # init: true
    ports:
      - "8800-8803:8000-8003"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #count: 1
              device_ids: ['0']
              capabilities: [gpu]
    environment:
      NGC_API_KEY: ""
#      DEBUG: 1
  ms-changenet:
    build:
      context: changenet
      target: builder_base
      dockerfile: Dockerfile
    image: ms-changenet
    volumes:
      - '~/.cache/nim/model-repo/visual_changenet:/workspace/model_repo/visual_changenet/1'
    ipc: host
    # init: true
    ports:
      - "8800-8803:8000-8003"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #count: 1
              device_ids: ['0']
              capabilities: [gpu]
    environment:
      DEBUG: 0
  nim-tao:
    user: nvs:1000
    build:
      context: tao
      target: nim_final
      dockerfile: Dockerfile.nim
      args:
        GITLAB_TOKEN: ${GITLAB_TOKEN}
    image: nim-tao
    # entrypoint: ["/bin/bash"]
    # # Keep container running
    # command: ["-c", "sleep infinity"]
    volumes:
      - '~/.cache/nim:/workspace/.cache'
      - './tao/validation/rtdetr/model_manifest.yaml:/opt/configs/model_manifest.rtdetr.yaml:ro'
    ipc: host
    # init: true
    ports:
      - "8800-8803:8000-8003"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #count: 1
              device_ids: ['0']
              capabilities: [gpu]
    environment:
      DEBUG: 0
      # NIM_MODEL_NAME: rtdetr
      NIM_MANIFEST_PATH: /opt/configs/model_manifest.rtdetr.yaml
      NGC_API_KEY: ${NGC_API_KEY}
  tao-cv:
    user: nvs:1000
    build:
      context: tao
      target: cv_tao_base
      dockerfile: Dockerfile
      args:
        GITLAB_TOKEN: ${GITLAB_TOKEN}
    image: tao-cv
    # entrypoint: ["/bin/bash"]
    # # Keep container running
    # command: ["-c", "sleep infinity"]
    volumes:
      - '~/.cache/nim:/workspace/.cache'
    ipc: host
    # init: true
    ports:
      - "8800:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #count: 1
              device_ids: ['0']
              capabilities: [gpu]
    environment:
      DEBUG: 0
      TAO_MODEL_NAME: rtdetr
  dry-run:
    user: nvs:1000
    build:
      context: dummy
      target: inference_base
      dockerfile: Dockerfile
      # used for creating NIM
#      dockerfile: Dockerfile.nim
    image: dry-run
    volumes:
      - '~/.cache/nim:/workspace/.cache'
    ipc: host
    # init: true
    ports:
      - "8800-8803:8000-8003"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #count: 1
              device_ids: ['0']
              capabilities: [gpu]
    environment:
      DEBUG: 1
  nim-qwen:
    build:
      context: qwen
      target: inference_base
      dockerfile: Dockerfile.nim
    image: nim-qwen
    volumes:
      - '~/.cache/nim:/opt/nim/.cache'
      - '$HOME/TensorRT-LLM:/workspace/TensorRT-LLM'
    ipc: host
    # init: true
    ports:
      - "8800-8803:8000-8003"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #count: 1
              device_ids: ['0']
              capabilities: [gpu]
    environment:
      DEBUG: 0
  ms-qwen:
    build:
      context: qwen
      target: inference_base
      dockerfile: Dockerfile
    image: ms-qwen
    volumes:
      - '~/.cache/nim/model-repo:/workspace/model-repo'
    ipc: host
    # init: true
    ports:
      - "8800-8803:8000-8003"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #count: 1
              device_ids: ['0']
              capabilities: [gpu]
    environment:
      DEBUG: 0
