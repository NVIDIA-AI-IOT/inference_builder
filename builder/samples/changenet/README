This sample demonstrates how to build a NIM inference pipeline for models that use triton backend.

Prepare the TRT engine file following instruction from https://gitlab-master.nvidia.com/dl/JoC/ai_foundation_models/-/blob/main/tao/visual-changenet/README.md?ref_type=heads
(Be sure to use tensorrt with the same version of that from the container image: 10.3.0.26)

Build the NIM inference flow:

cd builder
python main.py samples/changenet/trt_changenet.yaml -a samples/changenet/openapi.yaml -c samples/changenet/processors.py -o samples/changenet -t

Build and run the docker image:

cd samples
docker compose up --build nim-changenet

Test the NIM with a client:

cd builder/samples/changenet
python nim_client.py --host 127.0.0.1 --port 8803 --file test1.jpg test2.jpg