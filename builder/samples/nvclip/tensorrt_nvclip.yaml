name: nvclip

# top level inference input specification
input:
  - name: "text"
    data_type: TYPE_STRING
    dims: [ -1 ]
    optional: true
  - name: "image"
    data_type: TYPE_CUSTOM_IMAGE_BASE64
    dims: [ -1 ]
    optional: true
  - name: "indices"
    data_type: TYPE_STRING
    dims: [ -1 ]
    optional: true

# top level inference output specification
output:
  - name: "embeddings"
    data_type: TYPE_INT32
    dims: [-1, -1]
  - name: "total_tokens"
    data_type: TYPE_INT32
    dims: [ 1 ]
  - name: "num_images"
    data_type: TYPE_INT32
    dims: [ 1 ]
io_map:
  input:
    templates:
      # root template for generating input json from the request
      EmbeddingsRequest: >
        {
          "text": [
            {% set text_items = request.input | reject('startswith', 'data:image') | list %}
            {% for item in text_items %}
              {{ item|tojson }}{% if not loop.last %}, {% endif %}
            {% endfor %}
          ],
          "image": [
            {% set image_items = request.input | select('startswith', 'data:image') | map('remove_prefix', 'data:image/png;base64,') | list %}
            {% for item in image_items %}
              {{ item|tojson }}{% if not loop.last %}, {% endif %}
            {% endfor %}
          ],
          "indices": [
            {% for item in request.input %}
              "{{ 'image' if item.startswith('data:image') else 'text' }}"{% if not loop.last %}, {% endif %}
            {% endfor %}
          ]
        }
  output:
    templates:
      EmbeddingsResponse: >
        {
          "data": [{% for item in response.embeddings %} {"index": {{ loop.index }}, "embedding": {{ item|tojson }}, "object": "embedding"}{% if not loop.last %}, {% endif %}{% endfor %}],
          "usage": { "num_images": {{ response.num_images }}, "prompt_tokens": {{ response.total_tokens }}, "total_tokens": {{ response.total_tokens }}},
          "model": "nvidia/nvclip-vit-h-14",
          "object": "list"
        }

# list of model specifications for inference
models:
  - name: nvclip_clipa_vit_h14_700M_text
    backend: "triton/python/polygraphy"
    max_batch_size: 64
    input:
      - name: "TEXT"
        data_type: TYPE_INT64
        dims: [-1, -1]
    output:
      - name: "LOGITS_PER_TEXT"
        data_type: TYPE_INT32
        dims: [-1, -1]
    parameters:
      FORCE_CPU_ONLY_INPUT_TENSORS: "no"
    preprocessors:
      - kind: "custom"
        name: "openclip-tokenizer"
        model_path: "nvclip_clipa_vit_h14_700M_text/1"
        input: ["text"]
        output: ["TEXT"]
  - name: nvclip_clipa_vit_h14_700M_vision
    backend: "triton/python/polygraphy"
    max_batch_size: 64
    input:
      - name: "IMAGE"
        data_type: TYPE_FP32
        dims: [-1, 3, -1, -1]
    output:
      - name: "LOGITS_PER_IMAGE"
        data_type: TYPE_INT32
        dims: [-1, -1]
    parameters:
      FORCE_CPU_ONLY_INPUT_TENSORS: "no"
    preprocessors:
      - kind: "custom"
        name: "nvclip-vision-preprocessor"
        input: ["image"]
        output: ["IMAGE"]
post_processors:
  - kind: "custom"
    name: "nvclip-postprocessor"
    input: ["LOGITS_PER_TEXT", "LOGITS_PER_IMAGE", "indices"]
    output: ["embeddings", "total_tokens", "num_images"]
# route map
# A route is defined as <tensor source : tensor destination>
# Source and destination should be defined as <MODEL_NAME:["TENSOR_NAME1", "TENSOR_NAME2", ...]>
# Model name is optional, so <:["TENSOR_NAME1", "TENSOR_NAME2", ...]> is valid
# Tensor list is optional too and <my_model:> is valid
routes: {
  ':["text"]': 'nvclip_clipa_vit_h14_700M_text',
  ':["image"]': 'nvclip_clipa_vit_h14_700M_vision',
  ':["indices"]': ':',
  'nvclip_clipa_vit_h14_700M_text:["LOGITS_PER_TEXT"]': ':',
  'nvclip_clipa_vit_h14_700M_vision:["LOGITS_PER_IMAGE"]': ':'
}

# endpoint specification for server
endpoints:
  infer: "/v1/embeddings"
  health: "/health/live"

# environment specification
environment:
  tensorrt: 9.3.0