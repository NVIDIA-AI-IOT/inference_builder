This sample demostrats how to create the inference package for an embedding NIM with tensorrt backend (polygrapy) and fastapi server.

Before generating usable nvclip NIM, we need optimized tensorrt engines in hand. Please download the engines from NGC: https://registry.ngc.nvidia.com/orgs/nvstaging/teams/nim/models/nvclip-vit-h-14/files and save them to the cache directory(~/.cache/model-repo/nvclip/). Version 1.0.0 has been tested with inference builder.

The following files must be present in the cache directory:

- NVCLIP_224_700M_ViTH14.json
- nvclip_clipa_vit_h14_700M_text.plan
- nvclip_clipa_vit_h14_700M_vision.plan

Build the NIM inference flow:

cd builder
python main.py samples/nvclip/tensorrt_nvclip.yaml -a samples/nvclip/openapi.yaml -c samples/nvclip/processors.py -o samples/nvclip --server-type fastapi -t

Build and run the docker image:

cd samples
docker compose up --build nim-nvclip

Test the NIM with a client:

cd builder/samples/nvclip
./test_client.sh sample.png