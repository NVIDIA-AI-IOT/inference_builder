Before generating usable nvclip NIM, we need optimized tensorrt engines in hand. Please download the engines from NGC: https://registry.ngc.nvidia.com/orgs/nvstaging/teams/nim/models/nvclip-vit-h-14/files

Build the NIM inference flow:

cd builder
python main.py samples/nvclip/tensorrt_nvclip.yaml -a samples/nvclip/openapi.yaml -c samples/nvclip/processors.py -o samples/nvclip -t

Copy the trt engines and configuration for text and vision to samples/nvclip:

cp NVCLIP_224_700M_ViTH14.json samples/nvclip
cp nvclip_clipa_vit_h14_700M_text.plan samples/nvclip
cp nvclip_clipa_vit_h14_700M_vision.plan samples/nvclip


Build and run the docker image:

cd samples
docker compose up --build nim-nvclip

Test the NIM with a client:

cd builder/samples/nvclip
./test_client.sh sample.png