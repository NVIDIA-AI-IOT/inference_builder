# Introduction

This sample demonstrate how to create the inference microservice for an embedding model with TensorRT backend (polygrapy) and fastapi server.

We provide a sample Dockerfile for the example, which you can use to build a Docker image and test the microservice on any x86 system with an NVIDIA Ampere, Hopper, and Blackwell GPU.

# Prerequisites

**Note:** Make sure you are in the root directory (`path/to/inference-builder`) to execute the commands in this README. All relative paths and commands assume you are running from the inference-builder root directory. Also ensure that your virtual environment is activated before running any commands.

The model used in this sample can be found from NGC: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/nvclip_vit. Make sure you have the proper access right to the models, and download the checkpoints.

Before downloading the model file, you need to set up your model repository:

```bash
mkdir -p ~/.cache/model-repo && chmod 777 ~/.cache/model-repo
export MODEL_REPO=~/.cache/model-repo
```

**Note:** If NGC commands fail, make sure you have access to the models you are trying to download. Some models require an active subscription. Ensure NGC is set up properly, or alternatively try using the NGC web UI to directly download the model from the links provided above within the prerequisites section.

## Download the model file using NGC CLI:

```bash
ngc registry model download-version "nvidian/tao-nvaie/multi_modal_foundation_models:NVCLIP_224_700M_ViTH14" --dest /tmp
chmod 666 /tmp/multi_modal_foundation_models_vNVCLIP_224_700M_ViTH14/*
```

Run `ls /tmp/multi_modal_foundation_models_vNVCLIP_224_700M_ViTH14 -l` and be sure we have the checkpoint file `nvclip_clipa_vit_h14_700M.ckpt`.

## Create the optimized TensorRT Engine:

Prior to using the model for accelerated inference, we need to convert the checkpoints into onnx files and generate TensorRT engines accordingly.

First build the container image for TensorRT optimizer:

```bash
docker build -t trt-optimizer builder/samples/nvclip/optimizer
```

Then generate the engine files using the trt-optimizer

```bash
docker run -it --rm --gpus all \
           -v $MODEL_REPO:/workspace/checkpoints/optimized \
           -v /tmp/multi_modal_foundation_models_vNVCLIP_224_700M_ViTH14:/workspace/checkpoints/baseline \
           -e CHECKPOINT_NAME=nvclip_clipa_vit_h14_700M.ckpt \
           trt-optimizer
```

Run `ls $MODEL_REPO/`, If the above process runs correctly, there'll be 2 folders created under model-repo directory:
- nvclip_clipa_vit_h14_700M_vision: for vision encoder
- nvclip_clipa_vit_h14_700M_text: for text encoder

Go to inference-builder folder, adjust the permissions of each folder and copy the model config from source tree to the model folder:

```bash
sudo chmod 777 ~/.cache/model-repo/nvclip_clipa_vit_h14_700M_text ~/.cache/model-repo/nvclip_clipa_vit_h14_700M_vision
cp builder/samples/nvclip/optimizer/configs/* ~/.cache/model-repo/nvclip_clipa_vit_h14_700M_text
```

# Build the inference flow

Assume you've followed the [top level instructions](../../../README.md#getting-started) to set up the environment.

```bash
source .venv/bin/activate
python builder/main.py builder/samples/nvclip/tensorrt_nvclip.yaml -a builder/samples/nvclip/openapi.yaml -c builder/samples/nvclip/processors.py -o builder/samples/nvclip --server-type fastapi -t
```

# Build and run the docker image

```bash
cd builder/samples
docker compose up --build ms-nvclip
```

# Test the microservice with a client

After the server is successfully started, open a new terminal in your inference-builder folder to launch the client with a jpeg or png file. The embeddings generated by the model will be returned in the json payload.

**important:** **<sample.png> is a place holder, please replace it with the actual image path** 

```bash
source .venv/bin/activate && cd builder/samples/nvclip
./test_client.sh <sample.png>
```