# the name will be used to identify the microservice.
name: "qwen"
# the root path to the directory containing the model files (checkpoint, config, etc).
model_repo: "/opt/nim/.cache/model-repo"

# top level inference input specification
input:
  # messages from the http request
  - name: "messages"
    data_type: TYPE_STRING
    dims: [ -1 ]
  - name: "max_new_tokens"
    data_type: TYPE_INT32
    dims: [ 1 ]
output:
  - name: "text"
    data_type: TYPE_STRING
    dims: [ -1 ]

server:
  responders:
    infer:
      operation: create_chat_completion_v1_chat_completions_post
      requests:
        NIMLLMChatCompletionRequest: >
          {
            "messages": {{ request.messages|tojson }},
            "max_new_tokens": {{ request.max_tokens }}
          }
      responses:
        NIMLLMChatCompletionResponse: >
          {
            "id": "{{ request._id }}",
            "choices":
              [
                {% for text in response.text %}
                {
                  "index": {{ loop.index0 }},
                  "message": { "role": "assistant", "content": {{ text|tojson }} },
                  "finish_reason": "stop"
                }
                {% if not loop.last %}, {% endif %}
                {% endfor %}
              ],
            "model": "qwen-vl",
            "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0}
          }

models:
  - name: "Qwen2.5-VL-7B-Instruct"
    backend: "pytorch"
    input:
      - name: "input_ids"
        data_type: TYPE_INT64
        dims: [ -1, -1 ]
      - name: "attention_mask"
        data_type: TYPE_INT64
        dims: [ -1, -1 ]
      - name: "pixel_values"
        data_type: TYPE_FP32
        dims: [-1, -1]
      - name: "image_grid_thw"
        data_type: TYPE_INT64
        dims: [-1, -1]
      - name: "max_new_tokens"
        data_type: TYPE_INT32
        dims: [1]
    output:
      - name: "output_ids"
        data_type: TYPE_INT64
        dims: [ -1, -1 ]
    preprocessors:
      - kind: "custom"
        name: "qwen-vl-processor"
        input: ["messages", "max_new_tokens"]
        output: ["input_ids", "attention_mask", "pixel_values", "image_grid_thw", "max_new_tokens", "input_ids_copy"]
    postprocessors:
      - kind: "custom"
        name: "qwen-vl-tokenizer"
        input: ["output_ids", "input_ids_copy"]
        output: ["text"]
    parameters:
      model_class: "Qwen2_5_VLForConditionalGeneration"
