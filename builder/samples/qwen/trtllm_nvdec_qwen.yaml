# the name will be used to identify the microservice.
name: "qwen"
# the root path to the directory containing the model files (checkpoint, config, etc).
model_repo: "/opt/nim/.cache/model-repo"

# top level inference input specification
input:
  # messages from the http request
  - name: "prompts"
    data_type: TYPE_STRING
    dims: [ -1 ]
  - name : "videos"
    # This custom data type automatically converts the video chunk to a list of image Tensors
    data_type: TYPE_CUSTOM_VIDEO_CHUNK_ASSETS
    dims: [ -1 ]
    optional: true
  - name: "images"
    data_type: TYPE_CUSTOM_IMAGE_BASE64
    dims: [ -1 ]
    optional: true
  - name: "max_tokens"
    data_type: TYPE_INT32
    dims: [1]
  - name: "temperature"
    data_type: TYPE_FP32
    dims: [1]
    optional: true
  - name: "top_p"
    data_type: TYPE_FP32
    dims: [1]
    optional: true
  - name: "top_k"
    data_type: TYPE_INT32
    dims: [1]
    optional: true
output:
  - name: "outputs"
    data_type: TYPE_CUSTOM_OBJECT
    dims: [ -1 ]

server:
  responders:
    infer:
      operation: create_chat_completion_v1_chat_completions_post
      requests:
        NIMLLMChatCompletionRequest: >
          {
            "prompts": [
                {% for item in request.messages %}
                    {% if item.role == "user" %}
                        {% for content in item.content %}
                            {% if content.type == "text" %} {{ (content.text+"\n")|tojson }} {% endif %}
                        {% endfor %}
                    {% endif %}
                {% if not loop.last %}, {% endif %}
                {% endfor %}
            ],
            "videos": [
                {% for item in request.messages %}
                    {% if item.role == "user" %}
                        {% for content in item.content %}
                            {% if content.type == "video" %} {{ content.video|tojson }} {% endif %}
                        {% endfor %}
                    {% endif %}
                {% if not loop.last %}, {% endif %}
                {% endfor %}
            ],
            "images": [
                {% for item in request.messages %}
                    {% if item.role == "user" %}
                        {% for content in item.content %}
                            {% if content.type == "image" %} {{ content.image|tojson }} {% endif %}
                        {% endfor %}
                    {% endif %}
                {% if not loop.last %}, {% endif %}
                {% endfor %}
            ]
            {% if 'max_tokens' in request %}
            , "max_tokens": {{ request.max_tokens }}
            {% endif %}
            {% if 'temperature' in request %}
            , "temperature": {{ request.temperature }}
            {% endif %}
            {% if 'top_p' in request %}
            , "top_p": {{ request.top_p }}
            {% endif %}
            {% if 'top_k' in request %}
            , "top_k": {{ request.top_k }}
            {% endif %}
          }

      responses:
        NIMLLMChatCompletionResponse: >
          {
            {% set result = response.outputs[0] %}
            "id": "{{ result.request_id }}",
            "choices":
              [
                {% for output in result.outputs %}
                {
                  "index": {{ output.index }},
                  "message": { "role": "assistant", "content": {{ output.text|tojson }} },
                  "finish_reason": "stop"
                }
                {% if not loop.last %}, {% endif %}
                {% endfor %}
              ],
            "model": "qwen-vl",
            "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0}
          }
    add_file:
      operation: add_media_file
      responses:
        AddFileResponse: >
          {
            "data": {
              "id": {{response.id|tojson}},
              "path": {{response.path|tojson}},
              "size": {{response.size|tojson}},
              "duration": {{response.duration|tojson}},
              "contentType": {{response.mime_type|tojson}}
            }
          }
    del_file:
      operation: delete_media_file
      responses:
        DeleteFileResponse: >
          {
            "deleted": {{response.status|tojson}}
          }
    list_files:
      operation: list_media_files
      responses:
        ListFilesResponse: >
          {
            "data": [
              {% for item in response.assets %} {
                "id": {{item.id|tojson}},
                "path": {{item.path|tojson}},
                "size": {{item.size|tojson}},
                "contentType": {{item.mime_type|tojson}}
              } {% if not loop.last %}, {% endif %} {% endfor %}
            ]
          }
models:
  - name: "Qwen2.5-VL-7B-Instruct"
    backend: "tensorrtllm/pytorch"
    input:
      - name: "inputs"
        data_type: TYPE_CUSTOM_TRTLLM_INPUT
        dims: [ -1 ]
      - name: "max_tokens"
        data_type: TYPE_INT32
        dims: [1]
      - name: "temperature"
        data_type: TYPE_FP32
        dims: [1]
        optional: true
      - name: "top_p"
        data_type: TYPE_FP32
        dims: [1]
        optional: true
      - name: "top_k"
        data_type: TYPE_INT32
        dims: [1]
        optional: true
    output:
      - name: "outputs"
        data_type: TYPE_STRING
        dims: [ -1 ]
    parameters:
      max_num_tokens: 19200
      disable_kv_cache_reuse: true
    preprocessors:
      - kind: "custom"
        name: "qwen-vl-video-processor"
        input: ["prompts", "videos"]
        output: ["inputs"]
      - kind: "custom"
        name: "qwen-vl-image-processor"
        input: ["prompts", "images"]
        output: ["inputs"]
# TODO: add routes for video and image
routes: {
  ':["images", "prompts", "max_tokens", "temperature", "top_p", "top_k"]': 'Qwen2.5-VL-7B-Instruct',
  ':["videos", "prompts", "max_tokens", "temperature", "top_p", "top_k"]': 'Qwen2.5-VL-7B-Instruct',
  'Qwen2.5-VL-7B-Instruct:["outputs"]': ':'
}
