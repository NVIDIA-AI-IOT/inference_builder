# the name will be used to indicate the microservice.
name: dummy
# the root path to the directory containing the model files.
model_repo: /opt/nim/.cache/model-repo/

# top level inference input specification
input:
  - name: "text"
    data_type: TYPE_STRING
    dims: [ -1 ]
    optional: true
  - name: "images"
    # image data encoded as base64 string
    data_type: TYPE_CUSTOM_IMAGE_BASE64
    dims: [ -1 ]
    optional: true

# top level inference output specification
output:
  - name: "output"
    data_type: TYPE_FP32
    dims: [ -1, -1, -1 ]

server:
  responders:
    infer:
      operation: inference
      requests:
        InferenceRequest: >
          {
            {% set input_items = request.input %}
            {% if input_items|length > 0 %}
              {% if input_items[0] is string %}
                "images": [
                  {% for item in input_items %}
                      {{ item|tojson }}{% if not loop.last %}, {% endif %}
                  {% endfor %}
                ]
              {% elif input_items[0] is mapping %}
                "media_url": [
                  {% for item in input_items %}
                    {{ item.path|tojson }}{% if not loop.last %}, {% endif %}
                  {% endfor %}
                ]
              {% endif %}
              {% if request.text is not none %}
              ,
                "text": [
                  {% for item in request.text %}
                    {{ item|tojson }}{% if not loop.last %}, {% endif %}
                  {% endfor %}
                ]
              {% endif %}
            {% endif %}
          }
      responses:
        InferenceResponse: >
          {
            "data": [
              {% for item in response.output %} {
                "index": {{loop.index0}},
                "output": {{item | tojson}}
              } {% if not loop.last %}, {% endif %} {% endfor %} ],
            "usage": { "num_images": 1},
            "model": "nvidia/nvdinov2"
          }
    add_file:
      operation: add_media_file
      responses:
        AddFileResponse: >
          {
            "data": {
              "id": {{response.id|tojson}},
              "path": {{response.path|tojson}},
              "size": {{response.size|tojson}},
              "duration": {{response.duration|tojson}},
              "contentType": {{response.mime_type|tojson}}
            }
          }
    del_file:
      operation: delete_media_file
      responses:
        DeleteFileResponse: >
          {
            "deleted": {{response.status|tojson}}
          }
    list_files:
      operation: list_media_files
      responses:
        ListFilesResponse: >
          {
            "data": [
              {% for item in response.assets %} {
                "id": {{item.id|tojson}},
                "path": {{item.path|tojson}},
                "size": {{item.size|tojson}},
                "contentType": {{item.mime_type|tojson}}
              } {% if not loop.last %}, {% endif %} {% endfor %}
            ]
          }

models:
  - name: dummy
    backend: "dummy"
    max_batch_size: 1
    parameters:
      FORCE_CPU_ONLY_INPUT_TENSORS: "no"
    input:
      - name: images
        data_type: TYPE_FP32
        dims: [3, 768, 768]
      - name: input_ids
        data_type: TYPE_INT32
        dims: [ -1 ]
      - name: input_lengths
        data_type: TYPE_INT32
        dims: [ 1 ]
    output:
      - name: output
        data_type: TYPE_FP32
        dims: [10, 10]
    preprocessors:
      - kind: "custom"
        name: "dummy-preprocessor"
        input: ["images"]
        output: ["images"]
        config:
          network_size: [768, 768]
      - kind: "custom"
        name: "dummy-tokenizer"
        input: ["text"]
        output: ["input_ids", "input_lengths"]

