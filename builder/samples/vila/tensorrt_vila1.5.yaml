name: vila

# top level inference input specification
input:
  - name: "prompt"
    data_type: TYPE_STRING
    dims: [ 1 ]
  - name: "request_output_len"
    data_type: TYPE_INT32
    dims: [ 1 ]
  - name: "runtime_top_p"
    data_type: TYPE_FP32
    dims: [ 1 ]
  - name: "temperature"
    data_type: TYPE_FP32
    dims: [ 1 ]
  - name: "images"
    data_type: TYPE_CUSTOM_IMAGE_BASE64
    dims: [ -1 ]
  - name: "streaming"
    data_type: TYPE_BOOL
    dims: [ 1 ]
    optional: true
# top level inference output specification
output:
  - name: "text"
    data_type: TYPE_STRING
    dims: [ -1 ]

# top level data map from server to inference and vice versa
io_map:
  input:
    templates:
      # root template for generating input json from the request
      ChatRequest: >
        {
          "prompt": [{% for item in request.messages %}{% if item.role == "user" %}{{ item.content|tojson }}{% if not loop.last %},{% endif %}{% endif %}{% endfor %}],
          "runtime_top_p": {{ request.top_p }},
          "request_output_len": {{ request.max_tokens }},
          "temperature": {{ request.temperature }},
          "images": [],
          "streaming": true
        }
      # optional templates for fields
      prompt: "{% for prompt in data %}{{ prompt }}\n{% endfor %}"
    # optional filters for filtering each field with regex
    filters:
      prompt: ['(?P<src_base64_tag><img\ssrc=\"data:image\/(?P<extension>[^;]*);base64,(?P<base64str>[^\"]*)\".*?\/>)', '-', '', 'images']
  output:
    templates:
      # root template for formulating responses
      ChatCompletion: >
        {
          "id": "{{ request._id }}",
          "choices":
            [
              {
                "index": 0,
                "message": { "role": "assistant", "content": {{ response.text|tojson }} },
                "finish_reason": "stop"
              }
            ],
          "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0}
        }
      ChatCompletionChunk: >
        {
          "id": "{{ request._id }}",
          "choices":
            [
              {
                "index": 0,
                "delta": { "role": "assistant", "content": {{ response.text[0]|tojson }} },
                "finish_reason": "stop"
              }
            ]
        }


# list of model specifications for inference
models:
  - name: "vila1.5-13b"
    backend: "tensorrtllm"
    max_batch_size: 8
    model_transaction_policy:
      decoupled: True
    input:
      - name: "input_ids"
        data_type: TYPE_INT32
        dims: [ -1 ]
        allow_ragged_batch: true
      - name: "input_lengths"
        data_type: TYPE_INT32
        dims: [ 1 ]
      - name: "request_output_len"
        data_type: TYPE_INT32
        dims: [ 1 ]
      - name: "temperature"
        data_type: TYPE_FP32
        dims: [ 1 ]
        reshape: { shape: [ ] }
        optional: true
      - name: "runtime_top_p"
        data_type: TYPE_FP32
        dims: [ 1 ]
        reshape: { shape: [ ] }
        optional: true
      - name: "prompt_vocab_size"
        data_type: TYPE_UINT32
        dims: [ 1 ]
        reshape: { shape: [ ] }
        optional: true
      - name: "prompt_embedding_table"
        data_type: TYPE_FP16
        dims: [ -1, -1 ]
        optional: true
        allow_ragged_batch: true
      - name: "streaming"
        data_type: TYPE_BOOL
        dims: [ 1 ]
        optional: true
      - name: "end_id"
        data_type: TYPE_INT32
        dims: [ 1 ]
        reshape: { shape: [ ] }
        optional: true
      - name: "pad_id"
        data_type: TYPE_INT32
        dims: [ 1 ]
        reshape: { shape: [ ] }
        optional: true
    output:
      - name: "output_ids"
        data_type: TYPE_INT32
        dims: [ -1, -1 ]
      - name: "sequence_length"
        data_type: TYPE_INT32
        dims: [ -1 ]
    parameters:
      gpt_model_type: "inflight_fused_batching"
      gpt_model_path: "${engine_dir}"
    tokenizer:
      type: "auto"
      encoder: ["prompt", "input_ids"]
      decoder: ["output_ids", "output_ids"]
    preprocessors:
      - kind: "custom"
        name: "vila-muxer"
        input: ["input_ids", "features"]
        output: ["input_ids", "input_lengths", "prompt_vocab_size", "prompt_embedding_table"]
  - name: "visionenc"
    backend: "triton/python/tensorrtllm"
    max_batch_size: 8
    input:
      - name: "images"
        data_type: TYPE_FP16
        dims: [3, -1, -1]
    output:
      - name: "features"
        data_type: TYPE_FP16
        dims: [-1, -1]
        force_cpu: true
    parameters:
      FORCE_CPU_ONLY_INPUT_TENSORS: "no"
    instance_group:
      - count: 1
        kind: KIND_GPU
        gpus: [0]
    preprocessors:
      - kind: "custom"
        name: "vila-preprocessor"
        model_path: "vila1.5-13b"
        input: ["images"]
        output: ["images"]
    # only required if backend is tensorrt
    tensorrt_engine: "vila1.5-13b/fp16/1-gpu/visual_engines/visual_encoder.engine"


# route map
# A route is defined as <tensor source : tensor destination>
# Source and destination should be defined as <MODEL_NAME:["TENSOR_NAME1", "TENSOR_NAME2", ...]>
# Model name is optional, so <:["TENSOR_NAME1", "TENSOR_NAME2", ...]> is valid
# Tensor list is optional too and <my_model:> is valid
routes: {
  ':["prompt", "request_output_len", "temperature", "streaming"]': 'vila1.5-13b',
  ':["images"]': "visionenc",
  "visionenc": 'vila1.5-13b:',
  'vila1.5-13b:["output_ids"]': ':["text"]'
}

# endpoint specification for server
endpoints:
  infer: "/inference"
  health: "/health/live"

# environment specification
environment:
  tensorrt: 9.3.0