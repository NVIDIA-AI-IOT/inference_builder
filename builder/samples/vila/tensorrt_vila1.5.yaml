name: vila
model_repo: "/config/models/vila"

# top level inference input specification
input:
  - name: "prompt"
    data_type: TYPE_STRING
    dims: [ 1 ]
  - name: "request_output_len"
    data_type: TYPE_INT32
    dims: [ 1 ]
  - name: "runtime_top_p"
    data_type: TYPE_FP32
    dims: [ 1 ]
  - name: "temperature"
    data_type: TYPE_FP32
    dims: [ 1 ]
  - name: "images"
    data_type: TYPE_CUSTOM_IMAGE_BASE64
    dims: [ -1 ]
  - name: "streaming"
    data_type: TYPE_BOOL
    dims: [ 1 ]
    optional: true
# top level inference output specification
output:
  - name: "text"
    data_type: TYPE_STRING
    dims: [ -1 ]

# top level data map from server to inference and vice versa
server:
  responders:
    infer:
      operation: _infer_inference_post
      requests:
      # root template for generating input json from the request
        ChatRequest: >
          {
            "prompt": {% for item in request.messages %}{% if item.role == "user" %}{{ (item.content+"\n")|tojson }}{% endif %}{% endfor %},
            "runtime_top_p": {{ request.top_p }},
            "request_output_len": {{ request.max_tokens }},
            "temperature": {{ request.temperature }},
            "images": [],
            "streaming": false
          }
      # optional filters for constructing each field with regex
        prompt: ['(.*?)<img\s+src="(data:image/[^;]+;base64,[^"]+)"[^>]*\/?>', 'prompt', 'images']
      responses:
        ChatCompletion: >
          {
            "id": "{{ request._id }}",
            "choices":
              [
                {
                  "index": 0,
                  "message": { "role": "assistant", "content": {{ response.text|join('')|tojson }} },
                  "finish_reason": "stop"
                }
              ],
            "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0}
          }
        ChatCompletionChunk: >
          {
            "id": "{{ request._id }}",
            "choices":
              [
                {
                  "index": 0,
                  "delta": { "role": "assistant", "content": {{ response.text[0]|tojson }} },
                  "finish_reason": "stop"
                }
              ]
          }


# list of model specifications for inference
models:
  - name: "vila1.5-13b"
    backend: "triton/tensorrtllm"
    max_batch_size: 8
    model_transaction_policy:
      decoupled: True
    input:
      - name: "input_ids"
        data_type: TYPE_INT32
        dims: [ -1 ]
        allow_ragged_batch: true
      - name: "input_lengths"
        data_type: TYPE_INT32
        dims: [ 1 ]
      - name: "request_output_len"
        data_type: TYPE_INT32
        dims: [ 1 ]
      - name: "temperature"
        data_type: TYPE_FP32
        dims: [ 1 ]
        reshape: { shape: [ ] }
        optional: true
      - name: "runtime_top_p"
        data_type: TYPE_FP32
        dims: [ 1 ]
        reshape: { shape: [ ] }
        optional: true
      - name: "prompt_vocab_size"
        data_type: TYPE_UINT32
        dims: [ 1 ]
        reshape: { shape: [ ] }
        optional: true
      - name: "prompt_embedding_table"
        data_type: TYPE_FP16
        dims: [ -1, -1 ]
        optional: true
        allow_ragged_batch: true
      - name: "streaming"
        data_type: TYPE_BOOL
        dims: [ 1 ]
        optional: true
      - name: "end_id"
        data_type: TYPE_INT32
        dims: [ 1 ]
        reshape: { shape: [ ] }
        optional: true
      - name: "pad_id"
        data_type: TYPE_INT32
        dims: [ 1 ]
        reshape: { shape: [ ] }
        optional: true
    output:
      - name: "output_ids"
        data_type: TYPE_INT32
        dims: [ -1, -1 ]
      - name: "sequence_length"
        data_type: TYPE_INT32
        dims: [ -1 ]
    parameters:
      gpt_model_type: "inflight_fused_batching"
      gpt_model_path: "/config/models/vila/vila1.5-13b/fp16/1-gpu/"
    preprocessors:
      - kind: "custom"
        name: "vila-tokenizer"
        input: ["prompt"]
        output: ["input_ids"]
        config:
          is_encoder: true
      - kind: "custom"
        name: "vila-muxer"
        input: ["input_ids", "features"]
        output: ["input_ids", "input_lengths", "prompt_vocab_size", "prompt_embedding_table"]
    postprocessors:
      - kind: "custom"
        name: "vila-tokenizer"
        input: ["output_ids"]
        output: ["text"]
        config:
          is_encoder: false
          skip_special_tokens: true
  - name: "visionenc"
    backend: "triton/python/tensorrtllm"
    max_batch_size: 8
    input:
      - name: "images"
        data_type: TYPE_FP16
        dims: [3, -1, -1]
    output:
      - name: "features"
        data_type: TYPE_FP16
        dims: [-1, -1]
        force_cpu: true
    parameters:
      FORCE_CPU_ONLY_INPUT_TENSORS: "no"
      # use absolute path due to the non-standard file structure of VILA 1.5
      tensorrt_engine: "/config/models/vila/vila1.5-13b/fp16/1-gpu/visual_engines/visual_encoder.engine"
    instance_group:
      - count: 1
        kind: KIND_GPU
        gpus: [0]
    preprocessors:
      - kind: "custom"
        name: "vila-preprocessor"
        input: ["images"]
        output: ["images"]
        config:
          # override the model home directory due to the non-standard file structure of VILA 1.5
          model_home: "/config/models/vila/vila1.5-13b/"
    


# route map
# A route is defined as <tensor source : tensor destination>
# Source and destination should be defined as <MODEL_NAME:["TENSOR_NAME1", "TENSOR_NAME2", ...]>
# Model name is optional, so <:["TENSOR_NAME1", "TENSOR_NAME2", ...]> is valid
# Tensor list is optional too and <my_model:> is valid
routes: {
  ':["prompt", "request_output_len", "temperature", "streaming"]': 'vila1.5-13b',
  ':["images"]': "visionenc",
  'visionenc:["features"]': 'vila1.5-13b:',
  'vila1.5-13b:["text"]': ':["text"]'
}

# endpoint specification for server
endpoints:
  infer: "/inference"
  health: "/health/live"

# environment specification
environment:
  tensorrt: 9.3.0