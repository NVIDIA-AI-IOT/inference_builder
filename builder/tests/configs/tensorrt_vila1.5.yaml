name: vila
input: [
  {
    name: "prompt",
    data_type: TYPE_STRING,
    dims: [ 1 ]
  },
  {
      name: "request_output_len",
      data_type: TYPE_UINT32,
      dims: [ 1 ]
  },
  {
      name: "temperature",
      data_type: TYPE_FP32,
      dims: [ 1 ]
  },
  {
      name: "runtime_top_p",
      data_type: TYPE_FP32,
      dims: [ 1 ]
  }
]
output: [
  {
    name: "outputs",
    data_type: TYPE_STRING,
    dims: [ -1 ]
  }
]
models: [
  {
    name: "vila1.5-13b",
    backend: "tensorrtllm",
    input: [
      {
        name: "input_ids",
        data_type: TYPE_INT32,
        dims: [ -1 ]
      },
      {
        name: "input_lengths",
        data_type: TYPE_INT32,
        dims: [ 1 ]
      },
      {
        name: "request_output_len",
        data_type: TYPE_INT32,
        dims: [ 1 ]
      },
      {
        name: "temperature",
        data_type: TYPE_FP32,
        dims: [ 1 ],
        reshape: { shape: [ ] },
        optional: true
      },
      {
        name: "runtime_top_k",
        data_type: TYPE_INT32,
        dims: [ 1 ],
        reshape: { shape: [ ] },
        optional: true
      },
      {
        name: "runtime_top_p",
        data_type: TYPE_FP32,
        dims: [ 1 ],
        reshape: { shape: [ ] },
        optional: true
      },
      {
        name: "end_id",
        data_type: TYPE_INT32,
        dims: [ 1 ],
        reshape: { shape: [ ] },
        optional: true
      },
      {
        name: "pad_id",
        data_type: TYPE_INT32,
        dims: [ 1 ],
        reshape: { shape: [ ] },
        optional: true
      },
    ],
    output: [
      {
        name: "output_ids",
        data_type: TYPE_INT32,
        dims: [ -1, -1 ]
      },
      {
        name: "sequence_length",
        data_type: TYPE_INT32,
        dims: [ -1 ]
      }
    ],
    parameters: {
      "gpt_model_type": "inflight_fused_batching",
      "gpt_model_path": "${engine_dir}"
    }
  }
]

endpoints: {
  "infer": "/inference",
  "health": "/health/live"
}

input_map: {
  "prompt": '{% for item in data.messages %}{% if item.role == "user" %}{{ item.content }}{{ "\n" }}{% endif %}{% endfor %}',
  "runtime_top_p": "{{ data.top_p }}",
  "request_output_len": "{{ data.max_tokens }}"
}