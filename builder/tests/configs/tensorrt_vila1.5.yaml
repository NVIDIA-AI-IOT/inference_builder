name: vila
input: [
  {
    name: "prompt",
    data_type: TYPE_STRING,
    dims: [ 1 ]
  },
  {
      name: "request_output_len",
      data_type: TYPE_INT32,
      dims: [ 1 ]
  },
  {
      name: "runtime_top_p",
      data_type: TYPE_FP32,
      dims: [ 1 ]
  },
  {
      name: "temperature",
      data_type: TYPE_FP32,
      dims: [ 1 ]
  },
]
output: [
  {
    name: "text",
    data_type: TYPE_STRING,
    dims: [ -1 ]
  }
]
models: [
  {
    name: "vila1.5-13b",
    backend: "tensorrtllm",
    input: [
      {
        name: "input_ids",
        data_type: TYPE_INT32,
        dims: [ -1 ]
      },
      {
        name: "input_lengths",
        data_type: TYPE_INT32,
        dims: [ 1 ]
      },
      {
        name: "request_output_len",
        data_type: TYPE_INT32,
        dims: [ 1 ]
      },
      {
        name: "temperature",
        data_type: TYPE_FP32,
        dims: [ 1 ],
        reshape: { shape: [ ] },
        optional: true
      },
      {
        name: "runtime_top_p",
        data_type: TYPE_FP32,
        dims: [ 1 ],
        reshape: { shape: [ ] },
        optional: true
      }
    ],
    output: [
      {
        name: "output_ids",
        data_type: TYPE_INT32,
        dims: [ -1, -1 ]
      },
      {
        name: "sequence_length",
        data_type: TYPE_INT32,
        dims: [ -1 ]
      }
    ],
    parameters: {
      "gpt_model_type": "inflight_fused_batching",
      "gpt_model_path": "${engine_dir}"
    },
    tokenizer: {
      type: "auto",
      encoder: ["prompt", ["input_ids", "input_lengths"]],
      decoder: ["output_ids", "output_ids"]
    }
  }
]

routes : {
  ':["prompt", "request_output_len"]': 'vila1.5-13b',
  'vila1.5-13b:["output_ids"]': ':["text"]'
}

endpoints: {
  "infer": "/inference",
  "health": "/health/live"
}

input_map: {
  "prompt": '{% for item in data.messages %}{% if item.role == "user" %}{{ item.content }}{{ "\n" }}{% endif %}{% endfor %}',
  "runtime_top_p": "{{ data.top_p }}",
  "request_output_len": "{{ data.max_tokens }}"
}