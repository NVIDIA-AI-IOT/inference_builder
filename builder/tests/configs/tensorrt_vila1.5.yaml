name: vila
input: [
  {
    name: "prompt",
    data_type: TYPE_STRING,
    dims: [ 1 ]
  },
  {
      name: "request_output_len",
      data_type: TYPE_INT32,
      dims: [ 1 ]
  },
  {
      name: "runtime_top_p",
      data_type: TYPE_FP32,
      dims: [ 1 ]
  },
  {
      name: "temperature",
      data_type: TYPE_FP32,
      dims: [ 1 ]
  },
  {
    name: "images",
    data_type: TYPE_STRING,
    dims: [ -1 ]
  }
]
output: [
  {
    name: "text",
    data_type: TYPE_STRING,
    dims: [ -1 ]
  }
]
models: [
  {
    name: "vila1.5-13b",
    backend: "tensorrtllm",
    input: [
      {
        name: "input_ids",
        data_type: TYPE_INT32,
        dims: [ -1 ]
      },
      {
        name: "input_lengths",
        data_type: TYPE_INT32,
        dims: [ 1 ]
      },
      {
        name: "request_output_len",
        data_type: TYPE_INT32,
        dims: [ 1 ]
      },
      {
        name: "temperature",
        data_type: TYPE_FP32,
        dims: [ 1 ],
        reshape: { shape: [ ] },
        optional: true
      },
      {
        name: "runtime_top_p",
        data_type: TYPE_FP32,
        dims: [ 1 ],
        reshape: { shape: [ ] },
        optional: true
      }
    ],
    output: [
      {
        name: "output_ids",
        data_type: TYPE_INT32,
        dims: [ -1, -1 ]
      },
      {
        name: "sequence_length",
        data_type: TYPE_INT32,
        dims: [ -1 ]
      }
    ],
    parameters: {
      "gpt_model_type": "inflight_fused_batching",
      "gpt_model_path": "${engine_dir}"
    },
    tokenizer: {
      type: "auto",
      encoder: ["prompt", ["input_ids", "input_lengths"]],
      decoder: ["output_ids", "output_ids"]
    }
  }
]

routes : {
  ':["prompt", "request_output_len"]': 'vila1.5-13b',
  'vila1.5-13b:["output_ids"]': ':["text"]'
}

endpoints: {
  "infer": "/inference",
  "health": "/health/live"
}

projections:
  input:
    templates:
      request: >
        {
          "prompt": {% for item in messages %}[{% if item.role == "user" %}{{ item.content|tojson }}{% if not loop.last %},{% endif %}{% endif %}]{% endfor %},
          "runtime_top_p": {{ top_p }},
          "request_output_len": {{ max_tokens }},
          "temperature": {{ temperature }},
          "images": []
        }
      prompt: "{% for prompt in data %}{{ prompt }}\n{% endfor %}"
    filters: {
      "prompt": ['(?P<src_base64_tag><img\ssrc=\"data:image\/(?P<extension>[^;]*);base64,(?P<base64str>[^\"]*)\".*?\/>)', '-', '', 'images']
    }